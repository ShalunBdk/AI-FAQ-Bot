# -*- coding: utf-8 -*-
"""
–ú–æ–¥—É–ª—å –∫–∞—Å–∫–∞–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –æ—Ç–≤–µ—Ç–æ–≤ –≤ FAQ

–†–µ–∞–ª–∏–∑—É–µ—Ç 4-—É—Ä–æ–≤–Ω–µ–≤—É—é —Å–∏—Å—Ç–µ–º—É –ø–æ–∏—Å–∫–∞:
1. Exact Match - —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–∞
2. Keyword Search - –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (–¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤)
3. Semantic Search - —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ ChromaDB
4. Fallback - –≤–µ–∂–ª–∏–≤—ã–π –æ—Ç–∫–∞–∑ —Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏
"""

import logging
import re
from dataclasses import dataclass
from typing import Optional, Dict, List, Set

logger = logging.getLogger(__name__)

# ========== –°–¢–û–ü-–°–õ–û–í–ê –î–õ–Ø –†–£–°–°–ö–û–ì–û –Ø–ó–´–ö–ê ==========

RUSSIAN_STOP_WORDS = {
    '–≤', '–∏', '–Ω–∞', '—Å', '–ø–æ', '–∫', '–æ', '–æ—Ç', '–¥–ª—è', '–∏–∑', '—É', '–ø—Ä–∏',
    '—ç—Ç–æ', '–∫–∞–∫', '—á—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–∫—Ç–æ', '—á–µ–º', '–∂–µ', '–±—ã', '–ª–∏',
    '–∞', '–Ω–æ', '–∏–ª–∏', '–¥–∞', '–Ω–µ—Ç', '–Ω–µ', '–Ω–∏', '—Ç–æ', '—Ç–µ', '—ç—Ç–∏', '–≤—ã',
    '–º—ã', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–∏', '–æ–Ω–æ', '—è', '—Ç—ã', '–º–æ–π', '—Ç–≤–æ–π', '–µ–≥–æ',
    '–µ—ë', '–Ω–∞—à', '–≤–∞—à', '–∏—Ö', '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–æ', '–±—ã–ª–∏', '–µ—Å—Ç—å',
    '–±—ã—Ç—å', '–¥–µ–ª–∞—Ç—å', '—Å–¥–µ–ª–∞—Ç—å', '–º–æ—á—å', '—Ö–æ—Ç–µ—Ç—å', '—Å–∫–∞–∑–∞—Ç—å', '–≥–æ–≤–æ—Ä–∏—Ç—å',
    '–∑–Ω–∞—Ç—å', '—Å—Ç–∞—Ç—å', '–≤–∏–¥–µ—Ç—å', '—Ö–æ—Ä–æ—à–∏–π', '–Ω–æ–≤—ã–π', '–±–æ–ª—å—à–æ–π', '–¥—Ä—É–≥–æ–π'
}


# ========== –õ–ï–ú–ú–ê–¢–ò–ó–ê–¶–ò–Ø ==========

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä (lazy loading)
_morph_analyzer = None


def get_morph_analyzer():
    """
    –ü–æ–ª—É—á–∏—Ç—å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å –ª–µ–Ω–∏–≤–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π

    Returns:
        pymorphy3.MorphAnalyzer
    """
    global _morph_analyzer
    if _morph_analyzer is None:
        try:
            import pymorphy3
            _morph_analyzer = pymorphy3.MorphAnalyzer()
            logger.debug("–ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä pymorphy3 –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except ImportError:
            logger.warning("pymorphy3 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞.")
            _morph_analyzer = False  # –û—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ –ø–æ–ø—ã—Ç–∫–∞ –±—ã–ª–∞
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ pymorphy3: {e}")
            _morph_analyzer = False

    return _morph_analyzer if _morph_analyzer else None


def lemmatize_word(word: str) -> str:
    """
    –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞ (–ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∞—á–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ)

    –ü—Ä–∏–º–µ—Ä—ã:
        - –ø—Ä–µ—Ç–µ–Ω–∑–∏—é ‚Üí –ø—Ä–µ—Ç–µ–Ω–∑–∏—è
        - –ø—Ä–µ—Ç–µ–Ω–∑–∏–∏ ‚Üí –ø—Ä–µ—Ç–µ–Ω–∑–∏—è
        - —Å–æ—Å—Ç–∞–≤–∏—Ç—å ‚Üí —Å–æ—Å—Ç–∞–≤–∏—Ç—å
        - –Ω–∞—Ä—É—à–µ–Ω–∏—è ‚Üí –Ω–∞—Ä—É—à–µ–Ω–∏–µ

    Args:
        word: –°–ª–æ–≤–æ –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏

    Returns:
        –õ–µ–º–º–∞ (–Ω–∞—á–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞) –∏–ª–∏ –∏—Å—Ö–æ–¥–Ω–æ–µ —Å–ª–æ–≤–æ, –µ—Å–ª–∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞
    """
    morph = get_morph_analyzer()

    if not morph or not word:
        return word.lower()

    try:
        # –ü–∞—Ä—Å–∏–º —Å–ª–æ–≤–æ –∏ –±–µ—Ä–µ–º –ø–µ—Ä–≤—É—é (–Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—É—é) –ª–µ–º–º—É
        parsed = morph.parse(word)
        if parsed:
            lemma = parsed[0].normal_form
            return lemma.lower()
    except Exception as e:
        logger.debug(f"–û—à–∏–±–∫–∞ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–≤–∞ '{word}': {e}")

    return word.lower()


def lemmatize_text(text: str) -> List[str]:
    """
    –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (—Å–ø–∏—Å–æ–∫ –ª–µ–º–º –≤—Å–µ—Ö —Å–ª–æ–≤)

    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏

    Returns:
        –°–ø–∏—Å–æ–∫ –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤
    """
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç (—É–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏, –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ lowercase)
    normalized = normalize_text(text)
    words = normalized.split()

    # –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ
    lemmas = [lemmatize_word(word) for word in words]

    return lemmas


# ========== –ö–õ–ê–°–° –†–ï–ó–£–õ–¨–¢–ê–¢–ê –ü–û–ò–°–ö–ê ==========

@dataclass
class SearchResult:
    """
    –†–µ–∑—É–ª—å—Ç–∞—Ç –∫–∞—Å–∫–∞–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞

    Attributes:
        found: True –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç –Ω–∞–π–¥–µ–Ω, False –µ—Å–ª–∏ –Ω–µ—Ç
        faq_id: ID FAQ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        question: –¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞ –∏–∑ FAQ
        answer: –¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
        confidence: –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –æ—Ç–≤–µ—Ç–µ (0-100%)
        search_level: –£—Ä–æ–≤–µ–Ω—å –ø–æ–∏—Å–∫–∞ ('exact', 'keyword', 'semantic', 'none', 'direct')
        all_results: –ü–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ChromaDB (–¥–ª—è –ø–æ—Ö–æ–∂–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤)
        message: –°–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è fallback)
    """
    found: bool
    faq_id: Optional[str]
    question: Optional[str]
    answer: Optional[str]
    confidence: float
    search_level: str
    all_results: Optional[Dict]
    message: Optional[str]


# ========== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ==========

def normalize_text(text: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è

    –í—ã–ø–æ–ª–Ω—è–µ—Ç:
    - –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    - –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
    - –£–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    - –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤

    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç

    Returns:
        –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    if not text:
        return ""

    # –ù–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä
    text = text.lower().strip()

    # –£–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è (–∫—Ä–æ–º–µ –¥–µ—Ñ–∏—Å–∞ –∏ —Ç–æ—á–∫–∏ –≤ —á–∏—Å–ª–∞—Ö)
    text = re.sub(r'[^\w\s\-]', ' ', text)

    # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã ‚Üí –æ–¥–∏–Ω –ø—Ä–æ–±–µ–ª
    text = re.sub(r'\s+', ' ', text)

    return text.strip()


def extract_keywords(text: str, min_length: int = 3, use_lemmatization: bool = True) -> List[str]:
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–µ–π

    –ü—Ä–æ—Ü–µ—Å—Å:
    1. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (—É–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏, lowercase)
    2. –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å–ª–æ–≤ (–ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∞—á–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ)
    3. –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤
    4. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤

    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        min_length: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞
        use_lemmatization: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é (True –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)

    Returns:
        –°–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤

    Examples:
        >>> extract_keywords("–ö–∞–∫ —Å–æ—Å—Ç–∞–≤–∏—Ç—å –ø—Ä–µ—Ç–µ–Ω–∑–∏—é?")
        ['—Å–æ—Å—Ç–∞–≤–∏—Ç—å', '–ø—Ä–µ—Ç–µ–Ω–∑–∏—è']  # "—Å–æ—Å—Ç–∞–≤–∏—Ç—å" —É–∂–µ –≤ –Ω–∞—á–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ
        >>> extract_keywords("–£ –º–µ–Ω—è –ø—Ä–æ–±–ª–µ–º—ã —Å –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ–º")
        ['–ø—Ä–æ–±–ª–µ–º–∞', '–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ']  # "–ø—Ä–æ–±–ª–µ–º—ã" ‚Üí "–ø—Ä–æ–±–ª–µ–º–∞"
    """
    if use_lemmatization:
        # –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
        lemmas = lemmatize_text(text)
    else:
        # –û–±—ã—á–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–±–µ–∑ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏)
        normalized = normalize_text(text)
        lemmas = normalized.split()

    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
    keywords = [
        word for word in lemmas
        if word not in RUSSIAN_STOP_WORDS and len(word) >= min_length
    ]

    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
    seen = set()
    unique_keywords = []
    for kw in keywords:
        if kw not in seen:
            seen.add(kw)
            unique_keywords.append(kw)

    return unique_keywords


def calculate_keyword_confidence(
    matched_keywords: int,
    total_query_keywords: int,
    total_faq_keywords: int
) -> float:
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ confidence –¥–ª—è keyword search

    –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: —Å–∫–æ–ª—å–∫–æ —Å–ª–æ–≤ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞–π–¥–µ–Ω–æ.
    –§–æ—Ä–º—É–ª–∞ –ù–ï —à—Ç—Ä–∞—Ñ—É–µ—Ç FAQ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤.

    –õ–æ–≥–∏–∫–∞:
    - –í—Å–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞–π–¥–µ–Ω—ã ‚Üí 95% (–º–∞–∫—Å–∏–º—É–º –¥–ª—è keyword search)
    - –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ ‚Üí 70% + (–ø—Ä–æ—Ü–µ–Ω—Ç_—Å–æ–≤–ø–∞–¥–µ–Ω–∏–π * 25%)

    Args:
        matched_keywords: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–≤—à–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        total_query_keywords: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ
        total_faq_keywords: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ FAQ (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)

    Returns:
        Confidence (70-95%)
    """
    if total_query_keywords == 0:
        return 0.0

    # –ü—Ä–æ—Ü–µ–Ω—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å –∑–∞–ø—Ä–æ—Å–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–≥–ª–∞–≤–Ω—ã–π –∫—Ä–∏—Ç–µ—Ä–∏–π)
    query_match_ratio = matched_keywords / total_query_keywords

    # –ï—Å–ª–∏ –≤—Å–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞–π–¥–µ–Ω—ã ‚Üí –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π confidence
    if query_match_ratio == 1.0:
        return 95.0

    # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: –±–∞–∑–æ–≤—ã–π confidence 70% + –±–æ–Ω—É—Å –∑–∞ –∫–∞–∂–¥–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
    base_confidence = 70.0
    match_bonus = query_match_ratio * 25.0  # –î–æ +25% –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è

    confidence = base_confidence + match_bonus

    # –ú–∞–∫—Å–∏–º—É–º 95% –¥–ª—è keyword search (—á—Ç–æ–±—ã exact match –≤—Å–µ–≥–¥–∞ –±—ã–ª –ª—É—á—à–µ)
    return min(confidence, 95.0)


# ========== –£–†–û–í–ï–ù–¨ 1: EXACT MATCH ==========

def find_exact_match(query_text: str) -> Optional[SearchResult]:
    """
    –£—Ä–æ–≤–µ–Ω—å 1: –ü–æ–∏—Å–∫ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤–æ–ø—Ä–æ—Å–∞ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö

    –°–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π –º–µ—Ç–æ–¥ (O(1) —Å –∏–Ω–¥–µ–∫—Å–æ–º). –ò—â–µ—Ç –ø–æ–ª–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
    –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞.

    Args:
        query_text: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

    Returns:
        SearchResult —Å confidence=100% –∏–ª–∏ None
    """
    from src.core.database import get_db_connection

    normalized_query = normalize_text(query_text)

    if not normalized_query:
        return None

    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()

            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –≤–æ–ø—Ä–æ—Å—ã –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            cursor.execute("""
                SELECT id, category, question, answer, keywords
                FROM faq
            """)

            for row in cursor.fetchall():
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–æ–ø—Ä–æ—Å –∏–∑ –ë–î —Ç–µ–º –∂–µ —Å–ø–æ—Å–æ–±–æ–º
                normalized_db_question = normalize_text(row["question"])

                if normalized_db_question == normalized_query:
                    logger.debug(f"  [Exact Match] –ù–∞–π–¥–µ–Ω–æ: {row['question']}")
                    return SearchResult(
                        found=True,
                        faq_id=row["id"],
                        question=row["question"],
                        answer=row["answer"],
                        confidence=100.0,
                        search_level='exact',
                        all_results=None,
                        message=None
                    )

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ find_exact_match: {e}", exc_info=True)

    return None


# ========== –£–†–û–í–ï–ù–¨ 2: KEYWORD SEARCH ==========

def find_by_keywords(query_text: str, max_query_words: int = 5, threshold: float = 80.0) -> Optional[SearchResult]:
    """
    –£—Ä–æ–≤–µ–Ω—å 2: –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (—Ç–æ–ª—å–∫–æ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤)

    –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ —Ç–∏–ø–∞ "—Å–ø—Ä–∞–≤–∫–∞ 2-–ù–î–§–õ", "–∑–∞—Ä–ø–ª–∞—Ç–∞ –æ—Ç–ø—É—Å–∫"

    Args:
        query_text: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        max_query_words: –ú–∞–∫—Å–∏–º—É–º —Å–ª–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ –¥–ª—è keyword search
        threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 80%)

    Returns:
        SearchResult —Å confidence 80-95% –∏–ª–∏ None
    """
    from src.core.database import get_db_connection

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –∑–∞–ø—Ä–æ—Å–∞
    if len(query_text.split()) > max_query_words:
        logger.debug(f"  [Keyword Search] –ü—Ä–æ–ø—É—â–µ–Ω: –∑–∞–ø—Ä–æ—Å —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π ({len(query_text.split())} —Å–ª–æ–≤)")
        return None

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    query_keywords = extract_keywords(query_text)

    if not query_keywords:
        logger.debug("  [Keyword Search] –ü—Ä–æ–ø—É—â–µ–Ω: –Ω–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
        return None

    logger.debug(f"  [Keyword Search] –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {query_keywords}")

    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()

            # –°—Ç—Ä–æ–∏–º WHERE —É—Å–ª–æ–≤–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É
            # –ò—â–µ–º –≤ –≤–æ–ø—Ä–æ—Å–µ –ò –≤ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞—Ö
            conditions = []
            params = []

            for keyword in query_keywords:
                conditions.append(
                    "(LOWER(question) LIKE ? OR LOWER(keywords) LIKE ?)"
                )
                params.extend([f"%{keyword}%", f"%{keyword}%"])

            where_clause = " OR ".join(conditions)

            # –ó–∞–ø—Ä–æ—Å —Å –ø–æ–¥—Å—á–µ—Ç–æ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
            # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –æ–Ω–æ –≤ question –∏–ª–∏ keywords
            match_checks = []
            count_params = []
            for keyword in query_keywords:
                match_checks.append(
                    "(CASE WHEN LOWER(question) LIKE ? OR LOWER(keywords) LIKE ? THEN 1 ELSE 0 END)"
                )
                count_params.extend([f"%{keyword}%", f"%{keyword}%"])

            query_sql = f"""
                SELECT
                    id, category, question, answer, keywords,
                    ({' + '.join(match_checks)}) as match_count
                FROM faq
                WHERE {where_clause}
                ORDER BY match_count DESC
                LIMIT 1
            """

            # –ü–æ–ª–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ + –¥–ª—è WHERE
            full_params = count_params + params

            cursor.execute(query_sql, full_params)
            row = cursor.fetchone()

            if row and row["match_count"] > 0:
                # –í—ã—á–∏—Å–ª—è–µ–º confidence
                matched_keywords = row["match_count"]

                # –ü–æ–ª—É—á–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ FAQ
                faq_keywords_str = row["keywords"] or ""
                faq_keywords = [k.strip() for k in faq_keywords_str.split(",") if k.strip()]

                confidence = calculate_keyword_confidence(
                    matched_keywords=matched_keywords,
                    total_query_keywords=len(query_keywords),
                    total_faq_keywords=len(faq_keywords)
                )

                logger.debug(f"  [Keyword Search] –°–æ–≤–ø–∞–¥–µ–Ω–∏–π: {matched_keywords}/{len(query_keywords)}, confidence: {confidence:.1f}%")

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                if confidence >= threshold:
                    return SearchResult(
                        found=True,
                        faq_id=row["id"],
                        question=row["question"],
                        answer=row["answer"],
                        confidence=confidence,
                        search_level='keyword',
                        all_results=None,
                        message=None
                    )
                else:
                    logger.debug(f"  [Keyword Search] –û—Ç–∫–ª–æ–Ω–µ–Ω–æ: confidence {confidence:.1f}% < {threshold}%")

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ find_by_keywords: {e}", exc_info=True)

    return None


# ========== –£–†–û–í–ï–ù–¨ 3: SEMANTIC SEARCH ==========

def find_semantic_match(
    query_text: str,
    collection,
    threshold: float = 45.0,
    n_results: int = 3
) -> Optional[SearchResult]:
    """
    –£—Ä–æ–≤–µ–Ω—å 3: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ ChromaDB

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–º—ã—Å–ª–∞ –≤–æ–ø—Ä–æ—Å–∞.
    –°–∞–º—ã–π "—É–º–Ω—ã–π", –Ω–æ –∏ —Å–∞–º—ã–π –º–µ–¥–ª–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥.

    Args:
        query_text: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        collection: ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—è
        threshold: –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫)
        n_results: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    Returns:
        SearchResult —Å confidence >= threshold –∏–ª–∏ None
    """
    if collection is None:
        logger.error("ChromaDB collection –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        return None

    try:
        results = collection.query(
            query_texts=[query_text],
            n_results=n_results,
            include=["documents", "metadatas", "distances"]
        )

        if not results or not results['ids'] or not results['ids'][0]:
            logger.debug("  [Semantic Search] –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ ChromaDB")
            return None

        # –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        best_distance = results['distances'][0][0]
        similarity = max(0.0, 1.0 - best_distance) * 100.0
        best_metadata = results['metadatas'][0][0]
        faq_id = results['ids'][0][0]

        logger.debug(f"  [Semantic Search] –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: similarity={similarity:.1f}%, threshold={threshold}%")

        if similarity >= threshold:
            return SearchResult(
                found=True,
                faq_id=faq_id,
                question=best_metadata['question'],
                answer=best_metadata['answer'],
                confidence=similarity,
                search_level='semantic',
                all_results=results,
                message=None
            )
        else:
            logger.debug(f"  [Semantic Search] –û—Ç–∫–ª–æ–Ω–µ–Ω–æ: similarity {similarity:.1f}% < threshold {threshold}%")

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ find_semantic_match: {e}", exc_info=True)

    return None


# ========== –£–†–û–í–ï–ù–¨ 4: FALLBACK ==========

def get_fallback_result() -> SearchResult:
    """
    –£—Ä–æ–≤–µ–Ω—å 4: Fallback - –≤–µ–∂–ª–∏–≤—ã–π –æ—Ç–∫–∞–∑ —Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏

    –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –∫–æ–≥–¥–∞ –≤—Å–µ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —É—Ä–æ–≤–Ω–∏ –Ω–µ –Ω–∞—à–ª–∏ –æ—Ç–≤–µ—Ç.

    Returns:
        SearchResult —Å found=False –∏ —Å–æ–æ–±—â–µ–Ω–∏–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
    """
    from src.core.database import get_bot_setting

    # –ü–æ–ª—É—á–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    fallback_message = get_bot_setting("fallback_message")

    if not fallback_message:
        fallback_message = (
            "üòî –ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –Ω–∞—à–µ–ª —Ç–æ—á–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å.\n\n"
            "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ:\n"
            "‚Ä¢ –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å\n"
            "‚Ä¢ –í—ã–±—Ä–∞—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏–∑ —Å–ø–∏—Å–∫–∞\n"
            "‚Ä¢ –û–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–º—É —Å–æ—Ç—Ä—É–¥–Ω–∏–∫—É"
        )

    return SearchResult(
        found=False,
        faq_id=None,
        question=None,
        answer=None,
        confidence=0.0,
        search_level='none',
        all_results=None,
        message=fallback_message
    )


# ========== –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø: –ö–ê–°–ö–ê–î–ù–´–ô –ü–û–ò–°–ö ==========

def find_answer(
    query_text: str,
    collection,
    settings: Optional[Dict] = None
) -> SearchResult:
    """
    –ö–∞—Å–∫–∞–¥–Ω—ã–π –ø–æ–∏—Å–∫ –æ—Ç–≤–µ—Ç–∞ –ø–æ 4 —É—Ä–æ–≤–Ω—è–º

    –ü—Ä–æ—Ü–µ—Å—Å:
    1. –ü—ã—Ç–∞–µ—Ç—Å—è –Ω–∞–π—Ç–∏ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (exact match)
    2. –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏ –∑–∞–ø—Ä–æ—Å –∫–æ—Ä–æ—Ç–∫–∏–π - –∏—â–µ—Ç –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    3. –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ - —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ ChromaDB
    4. –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç fallback

    Args:
        query_text: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        collection: ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—è
        settings: –ù–∞—Å—Ç—Ä–æ–π–∫–∏ (–ø–æ—Ä–æ–≥–∏, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã). –ï—Å–ª–∏ None - –±–µ—Ä—É—Ç—Å—è –∏–∑ –ë–î

    Returns:
        SearchResult —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º –∏–ª–∏ fallback
    """
    from src.core.database import get_bot_settings

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã
    if settings is None:
        settings = get_bot_settings()

    # –ü–æ—Ä–æ–≥–∏ –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
    exact_threshold = float(settings.get('exact_match_threshold', 95))
    keyword_threshold = float(settings.get('keyword_match_threshold', 80))
    semantic_threshold = float(settings.get('semantic_match_threshold', 45))
    keyword_max_words = int(settings.get('keyword_search_max_words', 5))

    logger.info(f"üîç –ö–∞—Å–∫–∞–¥–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query_text}'")
    logger.debug(f"  –ü–æ—Ä–æ–≥–∏: exact={exact_threshold}%, keyword={keyword_threshold}%, semantic={semantic_threshold}%")

    # –£–†–û–í–ï–ù–¨ 1: Exact Match
    logger.debug("  –£—Ä–æ–≤–µ–Ω—å 1: –ü–æ–∏—Å–∫ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è...")
    result = find_exact_match(query_text)
    if result and result.confidence >= exact_threshold:
        logger.info(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ! Confidence: {result.confidence}%")
        return result

    # –£–†–û–í–ï–ù–¨ 2: Keyword Search (—Ç–æ–ª—å–∫–æ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤)
    if len(query_text.split()) <= keyword_max_words:
        logger.debug("  –£—Ä–æ–≤–µ–Ω—å 2: –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º...")
        result = find_by_keywords(query_text, max_query_words=keyword_max_words, threshold=keyword_threshold)
        if result and result.confidence >= keyword_threshold:
            logger.info(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º! Confidence: {result.confidence}%")
            return result
    else:
        logger.debug(f"  –£—Ä–æ–≤–µ–Ω—å 2: –ü—Ä–æ–ø—É—â–µ–Ω (–∑–∞–ø—Ä–æ—Å –¥–ª–∏–Ω–Ω—ã–π: {len(query_text.split())} —Å–ª–æ–≤ > {keyword_max_words})")

    # –£–†–û–í–ï–ù–¨ 3: Semantic Search
    logger.debug("  –£—Ä–æ–≤–µ–Ω—å 3: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫...")
    result = find_semantic_match(query_text, collection, threshold=semantic_threshold)
    if result:
        logger.info(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –ø–æ–∏—Å–∫–æ–º! Confidence: {result.confidence}%")
        return result

    # –£–†–û–í–ï–ù–¨ 4: Fallback
    logger.info("  ‚ùå –û—Ç–≤–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∏ –Ω–∞ –æ–¥–Ω–æ–º —É—Ä–æ–≤–Ω–µ. –í–æ–∑–≤—Ä–∞—â–∞–µ–º fallback.")
    return get_fallback_result()
