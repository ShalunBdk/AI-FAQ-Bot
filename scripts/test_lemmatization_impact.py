# -*- coding: utf-8 -*-
"""
–¢–µ—Å—Ç –≤–ª–∏—è–Ω–∏—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫

–¶–µ–ª—å: –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω–µ —É—Ö—É–¥—à–∞–µ—Ç –ª–∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
—Ç–æ—á–Ω–æ—Å—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø—Ä–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ö —Å–ª–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ.

–ú–µ—Ç–æ–¥–∏–∫–∞:
1. –ó–∞–≥—Ä—É–∂–∞–µ–º FAQ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ (–Ω–µ–ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏) –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
2. –°–æ–∑–¥–∞–µ–º –¥–≤–µ –≤–µ—Ä—Å–∏–∏ ChromaDB:
   - –í–µ—Ä—Å–∏—è A: –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
   - –í–µ—Ä—Å–∏—è B: –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
3. –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –≤ —Ä–∞–∑–Ω—ã—Ö —Å–ª–æ–≤–æ—Ñ–æ—Ä–º–∞—Ö
4. –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (similarity scores)
"""

import sys
import os
import logging
from typing import List, Dict, Tuple

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dotenv import load_dotenv
load_dotenv()

os.environ["ANONYMIZED_TELEMETRY"] = "False"

import chromadb
from chromadb.utils import embedding_functions

from src.core.database import get_all_faqs
from src.core.search import lemmatize_word, lemmatize_text, normalize_text

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(message)s'
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
MODEL_NAME = os.getenv("MODEL_NAME", "deepvk/USER2-base")
SIMILARITY_THRESHOLD = float(os.getenv("SIMILARITY_THRESHOLD", "45"))

# –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ö
TEST_QUERIES = [
    # –§–æ—Ä–º–∞—Ç: (–æ—Ä–∏–≥–∏–Ω–∞–ª, –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è, –æ–ø–∏—Å–∞–Ω–∏–µ)
    ("–∫–∞–∫ —Å–æ—Å—Ç–∞–≤–∏—Ç—å –ø—Ä–µ—Ç–µ–Ω–∑–∏—é?", "–∫–∞–∫ —Å–æ—Å—Ç–∞–≤–∏—Ç—å –ø—Ä–µ—Ç–µ–Ω–∑–∏—è?", "—Å–æ—Å—Ç–∞–≤–∏—Ç—å –ø—Ä–µ—Ç–µ–Ω–∑–∏—é"),
    ("—É –º–µ–Ω—è –ø—Ä–æ–±–ª–µ–º—ã —Å –ø–æ—á—Ç–æ–π", "—É —è –ø—Ä–æ–±–ª–µ–º–∞ —Å –ø–æ—á—Ç–∞", "–ø—Ä–æ–±–ª–µ–º—ã —Å –ø–æ—á—Ç–æ–π"),
    ("–Ω—É–∂–Ω–∞ —Å–ø—Ä–∞–≤–∫–∞ –æ –¥–æ—Ö–æ–¥–∞—Ö", "–Ω—É–∂–Ω—ã–π —Å–ø—Ä–∞–≤–∫–∞ –æ –¥–æ—Ö–æ–¥", "—Å–ø—Ä–∞–≤–∫–∞ –æ –¥–æ—Ö–æ–¥–∞—Ö"),
    ("–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∏–Ω—Ç–µ—Ä", "–Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å –ø—Ä–∏–Ω—Ç–µ—Ä", "–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∏–Ω—Ç–µ—Ä"),
    ("–æ—Ç–ø—Ä–∞–≤–∫–∞ –ø–∏—Å–µ–º –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç", "–æ—Ç–ø—Ä–∞–≤–∫–∞ –ø–∏—Å—å–º–æ –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å", "–æ—Ç–ø—Ä–∞–≤–∫–∞ –ø–∏—Å–µ–º"),
    ("—Ö–æ—á—É –ø–æ–¥–∞—Ç—å –∂–∞–ª–æ–±—É", "—Ö–æ—Ç–µ—Ç—å –ø–æ–¥–∞—Ç—å –∂–∞–ª–æ–±–∞", "–ø–æ–¥–∞—Ç—å –∂–∞–ª–æ–±—É"),
    ("—Ç—Ä–µ–±—É–µ—Ç—Å—è –∑–∞–º–µ–Ω–∞ –∫–∞—Ä—Ç—Ä–∏–¥–∂–∞", "—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –∑–∞–º–µ–Ω–∞ –∫–∞—Ä—Ç—Ä–∏–¥–∂", "–∑–∞–º–µ–Ω–∞ –∫–∞—Ä—Ç—Ä–∏–¥–∂–∞"),
    ("–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ", "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞", "–Ω–∞—Å—Ç—Ä–æ–π–∫–∞"),
    ("–ø—Ä–æ–±–ª–µ–º–∞ —Å –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π", "–ø—Ä–æ–±–ª–µ–º–∞ —Å –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è", "–∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è"),
    ("–∫–∞–∫ –æ—Ñ–æ—Ä–º–∏—Ç—å –≤–æ–∑–≤—Ä–∞—Ç —Ç–æ–≤–∞—Ä–∞?", "–∫–∞–∫ –æ—Ñ–æ—Ä–º–∏—Ç—å –≤–æ–∑–≤—Ä–∞—Ç —Ç–æ–≤–∞—Ä?", "–≤–æ–∑–≤—Ä–∞—Ç —Ç–æ–≤–∞—Ä–∞"),
]


def create_collection_with_keywords(
    client: chromadb.Client,
    collection_name: str,
    faqs: List[Dict],
    lemmatize_keywords: bool = False
) -> chromadb.Collection:
    """
    –°–æ–∑–¥–∞–µ—Ç ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å FAQ

    Args:
        client: ChromaDB –∫–ª–∏–µ–Ω—Ç
        collection_name: –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        faqs: –°–ø–∏—Å–æ–∫ FAQ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        lemmatize_keywords: –ï—Å–ª–∏ True, –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞

    Returns:
        ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—è
    """
    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é –µ—Å–ª–∏ –µ—Å—Ç—å
    try:
        client.delete_collection(name=collection_name)
    except:
        pass

    # –°–æ–∑–¥–∞–µ–º embedding function
    embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name=MODEL_NAME
    )

    # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
    collection = client.create_collection(
        name=collection_name,
        embedding_function=embedding_func,
        metadata={"hnsw:space": "cosine"}
    )

    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
    documents, metadatas, ids = [], [], []

    for faq in faqs:
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keywords_data = faq.get('keywords', '')

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö (–º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç—Ä–æ–∫–∞ –∏–ª–∏ —Å–ø–∏—Å–æ–∫)
        if isinstance(keywords_data, list):
            keywords_list = keywords_data
        elif isinstance(keywords_data, str):
            keywords_list = [kw.strip() for kw in keywords_data.split(',') if kw.strip()]
        else:
            keywords_list = []

        if keywords_list and lemmatize_keywords:
            # –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥–æ–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
            keywords_list = [lemmatize_word(kw) for kw in keywords_list]

        keywords = ' '.join(keywords_list) if keywords_list else ''

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
        text = f"{faq['question']} {keywords}"
        documents.append(f"search_document: {text}")

        metadatas.append({
            "category": faq["category"],
            "question": faq["question"],
            "answer": faq["answer"][:100] + "..." if len(faq["answer"]) > 100 else faq["answer"]
        })
        ids.append(str(faq["id"]))

    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é
    collection.add(documents=documents, metadatas=metadatas, ids=ids)

    logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –∫–æ–ª–ª–µ–∫—Ü–∏—è '{collection_name}' —Å {len(faqs)} FAQ")
    logger.info(f"   –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {'–î–∞' if lemmatize_keywords else '–ù–µ—Ç'}")

    return collection


def search_in_collection(
    collection: chromadb.Collection,
    query: str,
    n_results: int = 3
) -> List[Tuple[str, float, str]]:
    """
    –ü–æ–∏—Å–∫ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏

    Args:
        collection: ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—è
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        n_results: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    Returns:
        –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (question, similarity, faq_id)
    """
    try:
        results = collection.query(
            query_texts=[f"search_query: {query}"],
            n_results=n_results,
            include=["documents", "metadatas", "distances"]
        )

        if not results or not results['ids'] or not results['ids'][0]:
            return []

        output = []
        for i in range(len(results['ids'][0])):
            distance = results['distances'][0][i]
            similarity = max(0.0, 1.0 - distance) * 100.0
            metadata = results['metadatas'][0][i]
            faq_id = results['ids'][0][i]
            question = metadata['question']

            output.append((question, similarity, faq_id))

        return output

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
        return []


def compare_search_results(
    collection_original: chromadb.Collection,
    collection_lemmatized: chromadb.Collection,
    query_original: str,
    query_lemmatized: str,
    description: str
) -> Dict:
    """
    –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –≤ –¥–≤—É—Ö –∫–æ–ª–ª–µ–∫—Ü–∏—è—Ö

    Args:
        collection_original: –ö–æ–ª–ª–µ–∫—Ü–∏—è —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        collection_lemmatized: –ö–æ–ª–ª–µ–∫—Ü–∏—è —Å –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        query_original: –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å (—Å —Ä–∞–∑–Ω—ã–º–∏ —Ñ–æ—Ä–º–∞–º–∏ —Å–ª–æ–≤)
        query_lemmatized: –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        description: –û–ø–∏—Å–∞–Ω–∏–µ —Ç–µ—Å—Ç–∞

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    """
    logger.info(f"\n{'='*80}")
    logger.info(f"üìù –¢–µ—Å—Ç: {description}")
    logger.info(f"{'='*80}")

    # –ü–æ–∏—Å–∫ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º
    logger.info(f"\nüîç –ó–∞–ø—Ä–æ—Å (–æ—Ä–∏–≥–∏–Ω–∞–ª): '{query_original}'")

    results_orig_orig = search_in_collection(collection_original, query_original, n_results=3)
    results_orig_lem = search_in_collection(collection_lemmatized, query_original, n_results=3)

    logger.info(f"\n   üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞):")
    if results_orig_orig:
        for i, (question, similarity, faq_id) in enumerate(results_orig_orig, 1):
            logger.info(f"      {i}. [{similarity:.1f}%] {question}")
    else:
        logger.info(f"      ‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

    logger.info(f"\n   üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã (–ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞):")
    if results_orig_lem:
        for i, (question, similarity, faq_id) in enumerate(results_orig_lem, 1):
            logger.info(f"      {i}. [{similarity:.1f}%] {question}")
    else:
        logger.info(f"      ‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

    # –ü–æ–∏—Å–∫ —Å –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º
    logger.info(f"\nüîç –ó–∞–ø—Ä–æ—Å (–ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π): '{query_lemmatized}'")

    results_lem_orig = search_in_collection(collection_original, query_lemmatized, n_results=3)
    results_lem_lem = search_in_collection(collection_lemmatized, query_lemmatized, n_results=3)

    logger.info(f"\n   üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞):")
    if results_lem_orig:
        for i, (question, similarity, faq_id) in enumerate(results_lem_orig, 1):
            logger.info(f"      {i}. [{similarity:.1f}%] {question}")
    else:
        logger.info(f"      ‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

    logger.info(f"\n   üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã (–ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞):")
    if results_lem_lem:
        for i, (question, similarity, faq_id) in enumerate(results_lem_lem, 1):
            logger.info(f"      {i}. [{similarity:.1f}%] {question}")
    else:
        logger.info(f"      ‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

    # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–∏–π
    logger.info(f"\nüí° –ê–Ω–∞–ª–∏–∑:")

    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ç–æ–ø-1 —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
    if results_orig_orig and results_orig_lem:
        top1_orig = results_orig_orig[0]
        top1_lem = results_orig_lem[0]

        diff_similarity = top1_lem[1] - top1_orig[1]

        if abs(diff_similarity) < 1.0:
            logger.info(f"   ‚úÖ –†–∞–∑–Ω–∏—Ü–∞ –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞: {diff_similarity:+.1f}%")
        elif diff_similarity > 0:
            logger.info(f"   ‚úÖ –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —É–ª—É—á—à–∏–ª–∞ –ø–æ–∏—Å–∫ –Ω–∞ {diff_similarity:.1f}%")
        else:
            logger.info(f"   ‚ö†Ô∏è –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —É—Ö—É–¥—à–∏–ª–∞ –ø–æ–∏—Å–∫ –Ω–∞ {abs(diff_similarity):.1f}%")

        if top1_orig[2] != top1_lem[2]:
            logger.info(f"   ‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ò–∑–º–µ–Ω–∏–ª—Å—è —Ç–æ–ø-1 —Ä–µ–∑—É–ª—å—Ç–∞—Ç!")
            logger.info(f"      –ë–µ–∑ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏: {top1_orig[0]}")
            logger.info(f"      –° –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–µ–π: {top1_lem[0]}")

    return {
        'query_original': query_original,
        'query_lemmatized': query_lemmatized,
        'results_orig_orig': results_orig_orig,
        'results_orig_lem': results_orig_lem,
        'results_lem_orig': results_lem_orig,
        'results_lem_lem': results_lem_lem
    }


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""

    logger.info("="*80)
    logger.info("üß™ –¢–ï–°–¢ –í–õ–ò–Ø–ù–ò–Ø –õ–ï–ú–ú–ê–¢–ò–ó–ê–¶–ò–ò –ù–ê –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö")
    logger.info("="*80)
    logger.info(f"\nüìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    logger.info(f"   –ú–æ–¥–µ–ª—å: {MODEL_NAME}")
    logger.info(f"   –ü–æ—Ä–æ–≥ similarity: {SIMILARITY_THRESHOLD}%")
    logger.info(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤: {len(TEST_QUERIES)}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º FAQ –∏–∑ –±–∞–∑—ã
    logger.info(f"\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ FAQ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
    faqs = get_all_faqs()

    if not faqs:
        logger.error("‚ùå –í –±–∞–∑–µ –Ω–µ—Ç FAQ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è!")
        return

    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(faqs)} FAQ")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ FAQ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
    logger.info(f"\nüìù –ü—Ä–∏–º–µ—Ä—ã FAQ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏:")
    for i, faq in enumerate(faqs[:5], 1):
        keywords = faq.get('keywords', '')
        logger.info(f"   {i}. {faq['question']}")
        logger.info(f"      –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keywords if keywords else '(–Ω–µ—Ç)'}")

    # –°–æ–∑–¥–∞–µ–º ChromaDB –∫–ª–∏–µ–Ω—Ç (in-memory –¥–ª—è —Ç–µ—Å—Ç–æ–≤)
    logger.info(f"\nüîß –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∫–æ–ª–ª–µ–∫—Ü–∏–π...")
    client = chromadb.Client()

    # –°–æ–∑–¥–∞–µ–º –¥–≤–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
    collection_original = create_collection_with_keywords(
        client, "test_original", faqs, lemmatize_keywords=False
    )

    collection_lemmatized = create_collection_with_keywords(
        client, "test_lemmatized", faqs, lemmatize_keywords=True
    )

    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã
    logger.info(f"\nüöÄ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤...")

    all_results = []
    improvements = 0
    degradations = 0
    no_change = 0

    for query_original, query_lemmatized, description in TEST_QUERIES:
        result = compare_search_results(
            collection_original,
            collection_lemmatized,
            query_original,
            query_lemmatized,
            description
        )
        all_results.append(result)

        # –°—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        if result['results_orig_orig'] and result['results_orig_lem']:
            top1_orig = result['results_orig_orig'][0]
            top1_lem = result['results_orig_lem'][0]
            diff = top1_lem[1] - top1_orig[1]

            if abs(diff) < 1.0:
                no_change += 1
            elif diff > 0:
                improvements += 1
            else:
                degradations += 1

    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    logger.info(f"\n{'='*80}")
    logger.info(f"üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    logger.info(f"{'='*80}")
    logger.info(f"\n   –í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {len(TEST_QUERIES)}")
    logger.info(f"   ‚úÖ –£–ª—É—á—à–µ–Ω–∏–µ: {improvements} ({improvements/len(TEST_QUERIES)*100:.0f}%)")
    logger.info(f"   ‚ö†Ô∏è –£—Ö—É–¥—à–µ–Ω–∏–µ: {degradations} ({degradations/len(TEST_QUERIES)*100:.0f}%)")
    logger.info(f"   ‚ûñ –ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {no_change} ({no_change/len(TEST_QUERIES)*100:.0f}%)")

    # –í—ã–≤–æ–¥—ã
    logger.info(f"\nüí° –í–´–í–û–î–´:")

    if degradations > improvements:
        logger.info(f"   ‚ö†Ô∏è –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –£–•–£–î–®–ê–ï–¢ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫")
        logger.info(f"   –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤ –≤ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞—Ö")
    elif improvements > degradations:
        logger.info(f"   ‚úÖ –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –£–õ–£–ß–®–ê–ï–¢ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫")
        logger.info(f"   –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é")
    else:
        logger.info(f"   ‚ûñ –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –ù–ï –í–õ–ò–Ø–ï–¢ –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫")
        logger.info(f"   –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª—é–±–æ–π –ø–æ–¥—Ö–æ–¥")

    logger.info(f"\n" + "="*80)


if __name__ == "__main__":
    main()
